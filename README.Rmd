---
title: "Time Series Analysis and Modeling of the Healthyverse Packages"
output: github_document
always_allow_html: true
author: "Steven P. Sanderson II, MPH - Data Scientist/IT Manager"
date: "`r format(Sys.time(), '%d %B, %Y')`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    message = FALSE,
    warning = FALSE,
    fig.path = "man/figures/README-",
    fig.width = 12,
    fig.height = 10
)
source("00_scripts/load_libraries.R")
source("00_scripts/get_data_functions.R")
source("00_scripts/helper_functions.R")
source("00_scripts/data_manipulation_functions.R")
source("00_scripts/ts_decomp.R")
```

## Get Data

```{r get_data, echo=FALSE}
get_cran_data()
get_package_release_data()
csv_to_rds()
downloads_tbl <- downloads_processed_tbl()
pkg_tbl <- readRDS("01_data/pkg_release_tbl.rds") %>%
    mutate(date = as.Date(date))
```

```{r glimpse_data}
glimpse(downloads_tbl)
```

The last day in the data set is `r max_cran_datetime()`, the file was birthed on:
`r dl_birth_datetime()`, and at report knit time is `r hours_since_cran_log_update()` 
hours old. `r update_log_message()`

Now that we have our data lets take a look at it using the `skimr` package.

```{r skim_data}
skim(downloads_tbl)
```

We can see that the following columns are missing a lot of data and for us are most
likely not useful anyways, so we will drop them `c(r_version, r_arch, r_os)`

```{r data_trimmed, echo=FALSE}
data_tbl <- downloads_tbl %>%
    select(-r_version, -r_arch, -r_os)
```

## Plots

Now lets take a look at a time-series plot of the total daily downloads by package.
We will use a log scale and place a vertical line at each version release for each
package.

```{r initial_ts_plot, echo=FALSE}
md <- ts_downloads_tbl(data_tbl, "day", package) %>% 
  ungroup() %>% 
  select(date) %>% 
  distinct() %>% 
  filter(date == max(date)) %>%
  pull(date)

last_values <- ts_downloads_tbl(data_tbl, "day", package) %>% 
  ungroup() %>%
  filter(date == md)

ts_downloads_tbl(
    .data = data_tbl,
    .by_time = "day",
    package
) %>%
  filter(date >= subtract_time(md, "1 year")) %>%
  ggplot(aes(date, log1p(value))) +
  theme_bw() +
  geom_point(aes(group = package, color = package), size = 1) +
  ggtitle(paste("Package Downloads: {healthyverse}")) +
  geom_smooth(method = "loess", color = "black",  se = FALSE) +
  geom_vline(
    data = pkg_tbl
    , aes(xintercept = as.numeric(date))
    , color = "red"
    , lwd = 1
    , lty = "solid"
  ) +
  geom_point(
    shape = 21, size = 5, color = "red",
    data  = last_values,
    mapping = aes(x = date, y = log1p(value))
  ) +
  facet_wrap(package ~., ncol = 2, scales = "free_x") +
  theme_minimal() +
  labs(
    subtitle = "Vertical lines represent release dates",
    x = "Date",
    y = "log1p(Counts)",
    color = "Package"
  ) +
  theme(legend.position = "bottom")

ts_downloads_tbl(
    .data = data_tbl,
    .by_time = "day"
) %>%
  select(date, value) %>%
  summarise_by_time(
    .date_var = date,
    .by = "day",
    Actual = log1p(sum(value, na.rm = TRUE))
  ) %>%
  tk_augment_differences(.value = Actual, .differences = 1) %>%
  tk_augment_differences(.value = Actual, .differences = 2) %>%
  rename(velocity = contains("_diff1")) %>%
  rename(acceleration = contains("_diff2")) %>%
  pivot_longer(-date) %>%
  filter(date >= subtract_time(md, "1 year")) %>%
  mutate(name = str_to_title(name)) %>%
  mutate(name = as_factor(name)) %>%
  ggplot(aes(x = date, y = value, group = name)) +
  geom_point(alpha = .2) +
  geom_vline(
    data = pkg_tbl
    , aes(xintercept = date, color = package)
    , lwd = 1
    , lty = "solid"
  ) +
  facet_wrap(name ~ ., ncol = 1, scale = "free") +
  theme_minimal() +
  labs(
    title = "Total Downloads: Trend, Velocity, and Accelertion",
    subtitle = "Vertical Lines Indicate a CRAN Release date for a package.",
    x = "Date",
    y = "log1p(Values)",
    color = ""
  ) +
  theme(legend.position = "bottom")
```

Now lets take a look at some time series decomposition graphs.

```{r ts_decomp_plt, echo=FALSE}
plot_stl_diagnostics(
  .data = ts_downloads_tbl(
    .data = data_tbl,
    .by_time = "day"
    ),
  .date_var = date,
  .value = log1p(value),
  .interactive = FALSE
) +
  labs(
    title = "STL Diagnositcs: log1p(Values) - Daily Aggregation"
  ) +
  theme_minimal()

plot_stl_diagnostics(
  .data = ts_downloads_tbl(
    .data = data_tbl,
    .by_time = "month"
    ),
  .date_var = date,
  .value = log1p(value),
  .interactive = FALSE
) +
  labs(
    title = "STL Diagnositcs: log1p(Values) - Monthly Aggregation"
  ) +
  theme_minimal()

plot_seasonal_diagnostics(
    .data = ts_downloads_tbl(
    .data = data_tbl,
    .by_time = "day"
    ),
  .date_var = date,
  .value = log1p(value),
  .interactive = FALSE
) +
  labs(
    title = "Seasonal Diagnostics: log1p(Values)"
  ) +
  theme_minimal()

plot_acf_diagnostics(
      .data = ts_downloads_tbl(
    .data = data_tbl,
    .by_time = "day"
    ),
  .date_var = date,
  .value = log1p(value),
  .interactive = FALSE
) +
  labs(
    title = "Lag Diagnostics: log1p(Values)"
  ) +
  theme_minimal()
```

## Feature Engineering

Now that we have our basic data and a shot of what it looks like, let's add some
features to our data which can be very helpful in modeling. Lets start by making
a `tibble` that is aggregated by the day and package, as we are going to be interested
in forecasting the next 4 weeks or 28 days for each package. First lets get our base data.

```{r base_data_frame, echo=FALSE}
base_data <- ts_downloads_tbl(
  .data    = data_tbl,
  .by_time = "day",
  package
)
```

Lets glimpse it: `r glimpse(base_data)`

Now we are going to do some basic pre-processing.

```{r preprocess, message=FALSE}
data_padded_tbl <- base_data %>%
  pad_by_time(
    .date_var  = date,
    .pad_value = 0
  )

# Get log interval and standardization parameters
log_params  <- liv(data_padded_tbl$value, limit_lower = 0, offset = 1, silent = TRUE)
limit_lower <- log_params$limit_lower
limit_upper <- log_params$limit_upper
offset      <- log_params$offset
standard_params <- standard_vec(data_padded_tbl$value, silent = TRUE)
mean            <- standard_params$mean
sd              <- standard_params$sd
  
data_transformed_tbl <- data_padded_tbl %>%
  # Preprocess
  mutate(value_trans = liv(value, limit_lower = 0, offset = 1, silent = TRUE)$log_scaled) %>%
  mutate(value_trans = standard_vec(value_trans, silent = TRUE)$standard_scaled) %>%
  select(-value)
```

Now that we have our full data set and saved our parameters we can create the full
data set.

```{r data_prepared_full_tbl}
horizon <- 4*7
lag_period <- 4*7
rolling_periods <- c(7, 14, 28)

data_prepared_full_tbl <- data_transformed_tbl %>%
  group_by(package) %>%
  
  # Add future windows
  bind_rows(
    future_frame(., .date_var = date, .length_out = horizon)
  ) %>%
  
  # Add autocorolated lags
  tk_augment_lags(value_trans, .lags = lag_period) %>%
  
  # Add rolling features
  tk_augment_slidify(
    .value     = value_trans_lag28
    , .f       = median
    , .period  = rolling_periods
    , .align   = "center"
    , .partial = TRUE
  ) %>%
  
  # Format columns
  rename_with(.cols = contains("lag"), .fn = ~ str_c("lag_", .)) %>%
  select(date, package, everything()) %>%
  ungroup()

data_prepared_full_tbl %>% 
  group_by(package) %>% 
  pivot_longer(-c(date, package)) %>% 
  plot_time_series(
    .date_var = date
    , .value = value
    , .color_var = name
    , .smooth = FALSE
    , .interactive = FALSE
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

Since this is panel data we can follow one of two different modeling strategies. 
We can search for a global model in the panel data or we can use nested forecasting
finding the best model for each of the time series. Since we only have 5 panels, we
will use nested forecasting.

To do this we will use the `nest_timeseries` and `split_nested_timeseries` functions
to create a nested `tibble`.

```{r nested_data}
data_prepared_tbl <- data_prepared_full_tbl %>%
  filter(!is.na(value_trans))

forecast_tbl <- data_prepared_full_tbl %>%
  filter(is.na(value_trans))

nested_data_tbl <- data_prepared_tbl %>%
  nest_timeseries(
    .id_var = package
    , .length_future = horizon
  ) %>%
  split_nested_timeseries(
    .length_test = horizon
  )
```

Now it is time to make some recipes and models using the modeltime workflow.

## Modeltime Workflow

### Prophet Regression
We will first make a `progphet_reg` 

```{r propht_reg}
rec_prophet <- recipe(value_trans ~ date, extract_nested_test_split(nested_data_tbl))

wflw_prophet <- workflow() %>%
  add_model(
    prophet_reg(
      mode = "regression",
      seasonality_yearly = "auto",
      seasonality_weekly = "auto",
      seasonality_daily  = "auto"
    ) %>%
      set_engine("prophet")
  ) %>%
  add_recipe(rec_prophet)

wflw_prophet
```

### XGBoost Regression
We will use the `boost_tree` function

```{r xgboost}
rec_xgboost <- recipe(value_trans ~ date, extract_nested_test_split(nested_data_tbl)) %>%
  step_timeseries_signature(date) %>%
  step_rm(date) %>%
  step_zv(all_predictors()) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE)

wflw_xgboost <- workflow() %>%
  add_model(
    boost_tree(
      mode = "regression"
    ) %>%
      set_engine("xgboost")
  ) %>%
  add_recipe(rec_xgboost)

wflw_xgboost
```

### Nested Modeltime Tables
```{r nested_modeltime_tables}
nested_modeltime_tbl <- modeltime_nested_fit(
  # Nested Data
  nested_data = nested_data_tbl,
  
  # Add workflows
  wflw_prophet,
  wflw_xgboost
)

nested_modeltime_tbl
```

### Model Accuracy
```{r accuracy}
nested_modeltime_tbl %>%
  extract_nested_test_accuracy() %>%
  table_modeltime_accuracy(.interactive = F)
```

### Plot Models
```{r model_plot}
nested_modeltime_tbl %>%
  extract_nested_test_forecast() %>%
  group_by(package) %>%
  plot_modeltime_forecast(
    .interactive = FALSE,
    .conf_interval_alpha = .2
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

### Best Model
```{r best_model}
best_nested_modeltime_tbl <- nested_modeltime_tbl %>%
  modeltime_nested_select_best(
    metric = "rmse",
    minimize = TRUE,
    filter_test_forecasts = TRUE
  )

best_nested_modeltime_tbl %>%
  extract_nested_best_model_report()

best_nested_modeltime_tbl %>%
  extract_nested_test_forecast() %>%
  group_by(package) %>%
  plot_modeltime_forecast(
    .interactive = FALSE,
    .conf_interval_alpha = 0.2
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

## Refitting and Future Forecast

Now that we have the best models, we can make our future forecasts.

```{r refit}
nested_modeltime_refit_tbl <- best_nested_modeltime_tbl %>%
  modeltime_nested_refit(
    control = control_nested_refit(verbose = TRUE)
  )

nested_modeltime_refit_tbl
```